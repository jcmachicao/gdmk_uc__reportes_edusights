{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcmachicao/gdmk_uc__reportes_edusights/blob/main/ucont__mprecitivo_train_202502_tensorready.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAi6hmGNzcAC"
      },
      "source": [
        "# Entrenamiento de Red Neuronal Clasificación Binaria PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWpQ6t7bbRtS"
      },
      "source": [
        "## Preparación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9xMS9sEGsR5C"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXdbTKhUvPlC"
      },
      "source": [
        "https://docs.neptune.ai/setup/upgrading/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q8wX_236u5oi",
        "outputId": "48eb2df6-9b23-4938-b7cf-6d719d21e3dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.6/502.6 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for bravado-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q neptune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE2k8nFzusJY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from google.colab import userdata\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu9K6zbKJkbu",
        "outputId": "19552e2f-1ab2-49f4-ba97-cfd094a6786d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available.\n",
            "Device name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available.\")\n",
        "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"GPU is not available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph5VJmeBl6HL",
        "outputId": "2b6bd7ec-b5dd-468b-fc88-cf949c29af8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPr5bJoHLFMV",
        "outputId": "df8c4684-8b2b-4bd5-f8ee-8cbe5d6a7f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 24 17:53:46 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsCV3GMAxm-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd414af-907e-4705-9a4c-43bbb019b339"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['df_Ingresantes_uvir_2220al2410_0107.pkl',\n",
              " 'df_Ingresantes_uvir_2220al2410_abandono_mas_deserción.pkl',\n",
              " 'df_Ingresantes_uvir_2220al2410_deserción.pkl',\n",
              " 'Estudiantes_202420_al1107.xlsx',\n",
              " 'X_train.pt',\n",
              " 'y_train.pt',\n",
              " 'X_test.pt',\n",
              " 'y_test.pt']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "data_path = 'drive/My Drive/00 Aplicativos/data_aplicativos/archivos/'\n",
        "os.listdir(data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSw1z8plzUsx"
      },
      "source": [
        "## Rerun or Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeP5GlpI8HA0",
        "outputId": "b29f036a-fbca-48e2-c39e-27810b3addb5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8020, 149]), torch.Size([8020]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "X_tot = torch.load(data_path + 'X_train_tensor.pt')\n",
        "y_tot = torch.load(data_path + 'y_train_tensor.pt')\n",
        "X_tot.shape, y_tot.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_tot, y_tot, test_size=FRAC_TEST, random_state=None)"
      ],
      "metadata": {
        "id": "CTyxPChg_C2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(X_train, data_path + 'archivos/X_train.pt')\n",
        "torch.save(y_train, data_path + 'archivos/y_train.pt')\n",
        "torch.save(X_test, data_path + 'archivos/X_test.pt')\n",
        "torch.save(y_test, data_path + 'archivos/y_test.pt')"
      ],
      "metadata": {
        "id": "JnDWjC2T9pRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor = X_train.clone().requires_grad_(True).to(device)  # Create a tensor with requires_grad=True\n",
        "X_test_tensor = X_test.clone().requires_grad_(True).to(device)  # Create a tensor with requires_grad=True\n",
        "y_train_tensor = y_train.clone().requires_grad_(True).to(device)  # Create a tensor with requires_grad=True\n",
        "y_test_tensor = y_test.clone().requires_grad_(True).to(device)  # Create a tensor with requires_grad=True"
      ],
      "metadata": {
        "id": "OVeCOUV0_1rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fnmu0KIECClp",
        "outputId": "7d0c576f-5e52-4255-d546-91129165d5d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6416, 149]) torch.Size([6416])\n",
            "torch.Size([1604, 149]) torch.Size([1604])\n"
          ]
        }
      ],
      "source": [
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(X_train_tensor.shape, y_train_tensor.shape)\n",
        "print(X_test_tensor.shape, y_test_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedNN1(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(ImprovedNN1, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 32)\n",
        "        self.fc5 = nn.Linear(32, 1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.leaky_relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.leaky_relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.leaky_relu(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc4(x))\n",
        "        x = self.sigmoid(self.fc5(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "_nye5vQ39Fq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clWh_UNtFtQ6"
      },
      "source": [
        "### Tensores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MgKFM_nU-FO",
        "outputId": "29274847-b113-4353-e53e-3deeffc0f57f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño de Entrada:  149 Número de Clases de Salida:  2\n"
          ]
        }
      ],
      "source": [
        "input_size = X_train.shape[1]\n",
        "num_classes = len(np.unique(y_tot))\n",
        "print('Tamaño de Entrada: ', input_size, 'Número de Clases de Salida: ', num_classes)\n",
        "model = ImprovedNN1(input_size).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(model.parameters()).device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gI3RceaBKsYl",
        "outputId": "0f548d77-4e45-4e5d-bb05-48002004a846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSaFpgFKWY_h"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()  # or nn.BCEWithLogitsLoss() if you remove the sigmoid from the model\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=NUM_EPOCHS):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.squeeze()  # Squeeze to remove extra dimensions\n",
        "            loss = criterion(outputs, labels.float())  # Ensure labels are floats\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        run[\"train/loss\"].log(epoch_loss)  # Ensure 'run' is defined if you are using it for logging\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gz_Jt6SIFRU7"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_v2(model, test_data, test_labels):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(test_data)\n",
        "\n",
        "    outputs_show = outputs.cpu().numpy().flatten()\n",
        "    outputs_show[outputs_show > 0.51] = 1.0\n",
        "    outputs_show[outputs_show < 0.49] = 0.0\n",
        "    test_labels_np = test_labels.detach().cpu().numpy()\n",
        "\n",
        "    accuracy = 100 * (np.sum(outputs_show == test_labels_np) / len(test_labels_np))\n",
        "    print(f'Precisión del Modelo en Data de Prueba: {accuracy:.2f}%')\n",
        "\n",
        "    results_df = pd.DataFrame({\n",
        "        'Real': test_labels_np,\n",
        "        'Pred': outputs_show\n",
        "    })\n",
        "\n",
        "    return results_df, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "Y6on57q5ZIVe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4dB8VEgvESK9",
        "outputId": "5346d6f9-05d8-4bad-81cc-a31724eef473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [310/800], Loss: 0.0177\n",
            "Epoch [311/800], Loss: 0.0241\n",
            "Epoch [312/800], Loss: 0.0207\n",
            "Epoch [313/800], Loss: 0.0207\n",
            "Epoch [314/800], Loss: 0.0258\n",
            "Epoch [315/800], Loss: 0.0217\n",
            "Epoch [316/800], Loss: 0.0175\n",
            "Epoch [317/800], Loss: 0.0205\n",
            "Epoch [318/800], Loss: 0.0181\n",
            "Epoch [319/800], Loss: 0.0204\n",
            "Epoch [320/800], Loss: 0.0241\n",
            "Epoch [321/800], Loss: 0.0194\n",
            "Epoch [322/800], Loss: 0.0151\n",
            "Epoch [323/800], Loss: 0.0183\n",
            "Epoch [324/800], Loss: 0.0230\n",
            "Epoch [325/800], Loss: 0.0136\n",
            "Epoch [326/800], Loss: 0.0184\n",
            "Epoch [327/800], Loss: 0.0250\n",
            "Epoch [328/800], Loss: 0.0187\n",
            "Epoch [329/800], Loss: 0.0135\n",
            "Epoch [330/800], Loss: 0.0164\n",
            "Epoch [331/800], Loss: 0.0198\n",
            "Epoch [332/800], Loss: 0.0210\n",
            "Epoch [333/800], Loss: 0.0229\n",
            "Epoch [334/800], Loss: 0.0177\n",
            "Epoch [335/800], Loss: 0.0197\n",
            "Epoch [336/800], Loss: 0.0207\n",
            "Epoch [337/800], Loss: 0.0188\n",
            "Epoch [338/800], Loss: 0.0183\n",
            "Epoch [339/800], Loss: 0.0198\n",
            "Epoch [340/800], Loss: 0.0228\n",
            "Epoch [341/800], Loss: 0.0192\n",
            "Epoch [342/800], Loss: 0.0202\n",
            "Epoch [343/800], Loss: 0.0152\n",
            "Epoch [344/800], Loss: 0.0226\n",
            "Epoch [345/800], Loss: 0.0210\n",
            "Epoch [346/800], Loss: 0.0144\n",
            "Epoch [347/800], Loss: 0.0182\n",
            "Epoch [348/800], Loss: 0.0173\n",
            "Epoch [349/800], Loss: 0.0169\n",
            "Epoch [350/800], Loss: 0.0158\n",
            "Epoch [351/800], Loss: 0.0155\n",
            "Epoch [352/800], Loss: 0.0265\n",
            "Epoch [353/800], Loss: 0.0267\n",
            "Epoch [354/800], Loss: 0.0246\n",
            "Epoch [355/800], Loss: 0.0218\n",
            "Epoch [356/800], Loss: 0.0258\n",
            "Epoch [357/800], Loss: 0.0199\n",
            "Epoch [358/800], Loss: 0.0187\n",
            "Epoch [359/800], Loss: 0.0207\n",
            "Epoch [360/800], Loss: 0.0157\n",
            "Epoch [361/800], Loss: 0.0188\n",
            "Epoch [362/800], Loss: 0.0249\n",
            "Epoch [363/800], Loss: 0.0193\n",
            "Epoch [364/800], Loss: 0.0190\n",
            "Epoch [365/800], Loss: 0.0272\n",
            "Epoch [366/800], Loss: 0.0236\n",
            "Epoch [367/800], Loss: 0.0245\n",
            "Epoch [368/800], Loss: 0.0205\n",
            "Epoch [369/800], Loss: 0.0246\n",
            "Epoch [370/800], Loss: 0.0154\n",
            "Epoch [371/800], Loss: 0.0258\n",
            "Epoch [372/800], Loss: 0.0163\n",
            "Epoch [373/800], Loss: 0.0155\n",
            "Epoch [374/800], Loss: 0.0178\n",
            "Epoch [375/800], Loss: 0.0186\n",
            "Epoch [376/800], Loss: 0.0197\n",
            "Epoch [377/800], Loss: 0.0161\n",
            "Epoch [378/800], Loss: 0.0179\n",
            "Epoch [379/800], Loss: 0.0244\n",
            "Epoch [380/800], Loss: 0.0196\n",
            "Epoch [381/800], Loss: 0.0177\n",
            "Epoch [382/800], Loss: 0.0189\n",
            "Epoch [383/800], Loss: 0.0180\n",
            "Epoch [384/800], Loss: 0.0219\n",
            "Epoch [385/800], Loss: 0.0158\n",
            "Epoch [386/800], Loss: 0.0158\n",
            "Epoch [387/800], Loss: 0.0185\n",
            "Epoch [388/800], Loss: 0.0175\n",
            "Epoch [389/800], Loss: 0.0160\n",
            "Epoch [390/800], Loss: 0.0176\n",
            "Epoch [391/800], Loss: 0.0234\n",
            "Epoch [392/800], Loss: 0.0172\n",
            "Epoch [393/800], Loss: 0.0172\n",
            "Epoch [394/800], Loss: 0.0196\n",
            "Epoch [395/800], Loss: 0.0172\n",
            "Epoch [396/800], Loss: 0.0276\n",
            "Epoch [397/800], Loss: 0.0209\n",
            "Epoch [398/800], Loss: 0.0245\n",
            "Epoch [399/800], Loss: 0.0190\n",
            "Epoch [400/800], Loss: 0.0209\n",
            "Epoch [401/800], Loss: 0.0267\n",
            "Epoch [402/800], Loss: 0.0186\n",
            "Epoch [403/800], Loss: 0.0239\n",
            "Epoch [404/800], Loss: 0.0159\n",
            "Epoch [405/800], Loss: 0.0196\n",
            "Epoch [406/800], Loss: 0.0173\n",
            "Epoch [407/800], Loss: 0.0223\n",
            "Epoch [408/800], Loss: 0.0210\n",
            "Epoch [409/800], Loss: 0.0233\n",
            "Epoch [410/800], Loss: 0.0184\n",
            "Epoch [411/800], Loss: 0.0181\n",
            "Epoch [412/800], Loss: 0.0186\n",
            "Epoch [413/800], Loss: 0.0204\n",
            "Epoch [414/800], Loss: 0.0181\n",
            "Epoch [415/800], Loss: 0.0157\n",
            "Epoch [416/800], Loss: 0.0190\n",
            "Epoch [417/800], Loss: 0.0197\n",
            "Epoch [418/800], Loss: 0.0262\n",
            "Epoch [419/800], Loss: 0.0240\n",
            "Epoch [420/800], Loss: 0.0187\n",
            "Epoch [421/800], Loss: 0.0198\n",
            "Epoch [422/800], Loss: 0.0194\n",
            "Epoch [423/800], Loss: 0.0201\n",
            "Epoch [424/800], Loss: 0.0188\n",
            "Epoch [425/800], Loss: 0.0182\n",
            "Epoch [426/800], Loss: 0.0158\n",
            "Epoch [427/800], Loss: 0.0177\n",
            "Epoch [428/800], Loss: 0.0177\n",
            "Epoch [429/800], Loss: 0.0157\n",
            "Epoch [430/800], Loss: 0.0177\n",
            "Epoch [431/800], Loss: 0.0228\n",
            "Epoch [432/800], Loss: 0.0220\n",
            "Epoch [433/800], Loss: 0.0169\n",
            "Epoch [434/800], Loss: 0.0161\n",
            "Epoch [435/800], Loss: 0.0169\n",
            "Epoch [436/800], Loss: 0.0214\n",
            "Epoch [437/800], Loss: 0.0226\n",
            "Epoch [438/800], Loss: 0.0267\n",
            "Epoch [439/800], Loss: 0.0178\n",
            "Epoch [440/800], Loss: 0.0223\n",
            "Epoch [441/800], Loss: 0.0219\n",
            "Epoch [442/800], Loss: 0.0195\n",
            "Epoch [443/800], Loss: 0.0201\n",
            "Epoch [444/800], Loss: 0.0229\n",
            "Epoch [445/800], Loss: 0.0182\n",
            "Epoch [446/800], Loss: 0.0195\n",
            "Epoch [447/800], Loss: 0.0155\n",
            "Epoch [448/800], Loss: 0.0157\n",
            "Epoch [449/800], Loss: 0.0291\n",
            "Epoch [450/800], Loss: 0.0158\n",
            "Epoch [451/800], Loss: 0.0199\n",
            "Epoch [452/800], Loss: 0.0148\n",
            "Epoch [453/800], Loss: 0.0223\n",
            "Epoch [454/800], Loss: 0.0190\n",
            "Epoch [455/800], Loss: 0.0233\n",
            "Epoch [456/800], Loss: 0.0153\n",
            "Epoch [457/800], Loss: 0.0168\n",
            "Epoch [458/800], Loss: 0.0189\n",
            "Epoch [459/800], Loss: 0.0198\n",
            "Epoch [460/800], Loss: 0.0217\n",
            "Epoch [461/800], Loss: 0.0164\n",
            "Epoch [462/800], Loss: 0.0192\n",
            "Epoch [463/800], Loss: 0.0253\n",
            "Epoch [464/800], Loss: 0.0239\n",
            "Epoch [465/800], Loss: 0.0221\n",
            "Epoch [466/800], Loss: 0.0184\n",
            "Epoch [467/800], Loss: 0.0256\n",
            "Epoch [468/800], Loss: 0.0173\n",
            "Epoch [469/800], Loss: 0.0144\n",
            "Epoch [470/800], Loss: 0.0162\n",
            "Epoch [471/800], Loss: 0.0193\n",
            "Epoch [472/800], Loss: 0.0198\n",
            "Epoch [473/800], Loss: 0.0204\n",
            "Epoch [474/800], Loss: 0.0197\n",
            "Epoch [475/800], Loss: 0.0175\n",
            "Epoch [476/800], Loss: 0.0176\n",
            "Epoch [477/800], Loss: 0.0219\n",
            "Epoch [478/800], Loss: 0.0148\n",
            "Epoch [479/800], Loss: 0.0217\n",
            "Epoch [480/800], Loss: 0.0157\n",
            "Epoch [481/800], Loss: 0.0177\n",
            "Epoch [482/800], Loss: 0.0123\n",
            "Epoch [483/800], Loss: 0.0130\n",
            "Epoch [484/800], Loss: 0.0183\n",
            "Epoch [485/800], Loss: 0.0189\n",
            "Epoch [486/800], Loss: 0.0196\n",
            "Epoch [487/800], Loss: 0.0246\n",
            "Epoch [488/800], Loss: 0.0213\n",
            "Epoch [489/800], Loss: 0.0226\n",
            "Epoch [490/800], Loss: 0.0189\n",
            "Epoch [491/800], Loss: 0.0224\n",
            "Epoch [492/800], Loss: 0.0192\n",
            "Epoch [493/800], Loss: 0.0159\n",
            "Epoch [494/800], Loss: 0.0153\n",
            "Epoch [495/800], Loss: 0.0212\n",
            "Epoch [496/800], Loss: 0.0201\n",
            "Epoch [497/800], Loss: 0.0163\n",
            "Epoch [498/800], Loss: 0.0226\n",
            "Epoch [499/800], Loss: 0.0253\n",
            "Epoch [500/800], Loss: 0.0206\n",
            "Epoch [501/800], Loss: 0.0238\n",
            "Epoch [502/800], Loss: 0.0186\n",
            "Epoch [503/800], Loss: 0.0188\n",
            "Epoch [504/800], Loss: 0.0203\n",
            "Epoch [505/800], Loss: 0.0159\n",
            "Epoch [506/800], Loss: 0.0177\n",
            "Epoch [507/800], Loss: 0.0225\n",
            "Epoch [508/800], Loss: 0.0163\n",
            "Epoch [509/800], Loss: 0.0209\n",
            "Epoch [510/800], Loss: 0.0201\n",
            "Epoch [511/800], Loss: 0.0184\n",
            "Epoch [512/800], Loss: 0.0166\n",
            "Epoch [513/800], Loss: 0.0176\n",
            "Epoch [514/800], Loss: 0.0182\n",
            "Epoch [515/800], Loss: 0.0197\n",
            "Epoch [516/800], Loss: 0.0220\n",
            "Epoch [517/800], Loss: 0.0204\n",
            "Epoch [518/800], Loss: 0.0124\n",
            "Epoch [519/800], Loss: 0.0219\n",
            "Epoch [520/800], Loss: 0.0284\n",
            "Epoch [521/800], Loss: 0.0191\n",
            "Epoch [522/800], Loss: 0.0180\n",
            "Epoch [523/800], Loss: 0.0133\n",
            "Epoch [524/800], Loss: 0.0171\n",
            "Epoch [525/800], Loss: 0.0186\n",
            "Epoch [526/800], Loss: 0.0174\n",
            "Epoch [527/800], Loss: 0.0194\n",
            "Epoch [528/800], Loss: 0.0184\n",
            "Epoch [529/800], Loss: 0.0165\n",
            "Epoch [530/800], Loss: 0.0209\n",
            "Epoch [531/800], Loss: 0.0157\n",
            "Epoch [532/800], Loss: 0.0151\n",
            "Epoch [533/800], Loss: 0.0180\n",
            "Epoch [534/800], Loss: 0.0168\n",
            "Epoch [535/800], Loss: 0.0180\n",
            "Epoch [536/800], Loss: 0.0184\n",
            "Epoch [537/800], Loss: 0.0218\n",
            "Epoch [538/800], Loss: 0.0152\n",
            "Epoch [539/800], Loss: 0.0209\n",
            "Epoch [540/800], Loss: 0.0191\n",
            "Epoch [541/800], Loss: 0.0157\n",
            "Epoch [542/800], Loss: 0.0149\n",
            "Epoch [543/800], Loss: 0.0165\n",
            "Epoch [544/800], Loss: 0.0186\n",
            "Epoch [545/800], Loss: 0.0180\n",
            "Epoch [546/800], Loss: 0.0207\n",
            "Epoch [547/800], Loss: 0.0133\n",
            "Epoch [548/800], Loss: 0.0177\n",
            "Epoch [549/800], Loss: 0.0171\n",
            "Epoch [550/800], Loss: 0.0223\n",
            "Epoch [551/800], Loss: 0.0153\n",
            "Epoch [552/800], Loss: 0.0181\n",
            "Epoch [553/800], Loss: 0.0226\n",
            "Epoch [554/800], Loss: 0.0147\n",
            "Epoch [555/800], Loss: 0.0127\n",
            "Epoch [556/800], Loss: 0.0218\n",
            "Epoch [557/800], Loss: 0.0190\n",
            "Epoch [558/800], Loss: 0.0180\n",
            "Epoch [559/800], Loss: 0.0147\n",
            "Epoch [560/800], Loss: 0.0134\n",
            "Epoch [561/800], Loss: 0.0201\n",
            "Epoch [562/800], Loss: 0.0262\n",
            "Epoch [563/800], Loss: 0.0248\n",
            "Epoch [564/800], Loss: 0.0192\n",
            "Epoch [565/800], Loss: 0.0222\n",
            "Epoch [566/800], Loss: 0.0166\n",
            "Epoch [567/800], Loss: 0.0216\n",
            "Epoch [568/800], Loss: 0.0192\n",
            "Epoch [569/800], Loss: 0.0190\n",
            "Epoch [570/800], Loss: 0.0197\n",
            "Epoch [571/800], Loss: 0.0167\n",
            "Epoch [572/800], Loss: 0.0138\n",
            "Epoch [573/800], Loss: 0.0154\n",
            "Epoch [574/800], Loss: 0.0173\n",
            "Epoch [575/800], Loss: 0.0172\n",
            "Epoch [576/800], Loss: 0.0230\n",
            "Epoch [577/800], Loss: 0.0183\n",
            "Epoch [578/800], Loss: 0.0182\n",
            "Epoch [579/800], Loss: 0.0223\n",
            "Epoch [580/800], Loss: 0.0202\n",
            "Epoch [581/800], Loss: 0.0125\n",
            "Epoch [582/800], Loss: 0.0179\n",
            "Epoch [583/800], Loss: 0.0143\n",
            "Epoch [584/800], Loss: 0.0173\n",
            "Epoch [585/800], Loss: 0.0138\n",
            "Epoch [586/800], Loss: 0.0145\n",
            "Epoch [587/800], Loss: 0.0151\n",
            "Epoch [588/800], Loss: 0.0180\n",
            "Epoch [589/800], Loss: 0.0174\n",
            "Epoch [590/800], Loss: 0.0213\n",
            "Epoch [591/800], Loss: 0.0214\n",
            "Epoch [592/800], Loss: 0.0180\n",
            "Epoch [593/800], Loss: 0.0262\n",
            "Epoch [594/800], Loss: 0.0174\n",
            "Epoch [595/800], Loss: 0.0252\n",
            "Epoch [596/800], Loss: 0.0209\n",
            "Epoch [597/800], Loss: 0.0186\n",
            "Epoch [598/800], Loss: 0.0201\n",
            "Epoch [599/800], Loss: 0.0180\n",
            "Epoch [600/800], Loss: 0.0175\n",
            "Epoch [601/800], Loss: 0.0190\n",
            "Epoch [602/800], Loss: 0.0170\n",
            "Epoch [603/800], Loss: 0.0165\n",
            "Epoch [604/800], Loss: 0.0219\n",
            "Epoch [605/800], Loss: 0.0228\n",
            "Epoch [606/800], Loss: 0.0131\n",
            "Epoch [607/800], Loss: 0.0148\n",
            "Epoch [608/800], Loss: 0.0192\n",
            "Epoch [609/800], Loss: 0.0158\n",
            "Epoch [610/800], Loss: 0.0171\n",
            "Epoch [611/800], Loss: 0.0221\n",
            "Epoch [612/800], Loss: 0.0163\n",
            "Epoch [613/800], Loss: 0.0170\n",
            "Epoch [614/800], Loss: 0.0181\n",
            "Epoch [615/800], Loss: 0.0143\n",
            "Epoch [616/800], Loss: 0.0171\n",
            "Epoch [617/800], Loss: 0.0169\n",
            "Epoch [618/800], Loss: 0.0176\n",
            "Epoch [619/800], Loss: 0.0169\n",
            "Epoch [620/800], Loss: 0.0192\n",
            "Epoch [621/800], Loss: 0.0205\n",
            "Epoch [622/800], Loss: 0.0176\n",
            "Epoch [623/800], Loss: 0.0177\n",
            "Epoch [624/800], Loss: 0.0176\n",
            "Epoch [625/800], Loss: 0.0204\n",
            "Epoch [626/800], Loss: 0.0162\n",
            "Epoch [627/800], Loss: 0.0196\n",
            "Epoch [628/800], Loss: 0.0157\n",
            "Epoch [629/800], Loss: 0.0149\n",
            "Epoch [630/800], Loss: 0.0225\n",
            "Epoch [631/800], Loss: 0.0209\n",
            "Epoch [632/800], Loss: 0.0226\n",
            "Epoch [633/800], Loss: 0.0183\n",
            "Epoch [634/800], Loss: 0.0193\n",
            "Epoch [635/800], Loss: 0.0178\n",
            "Epoch [636/800], Loss: 0.0222\n",
            "Epoch [637/800], Loss: 0.0169\n",
            "Epoch [638/800], Loss: 0.0150\n",
            "Epoch [639/800], Loss: 0.0153\n",
            "Epoch [640/800], Loss: 0.0229\n",
            "Epoch [641/800], Loss: 0.0174\n",
            "Epoch [642/800], Loss: 0.0235\n",
            "Epoch [643/800], Loss: 0.0201\n",
            "Epoch [644/800], Loss: 0.0162\n",
            "Epoch [645/800], Loss: 0.0171\n",
            "Epoch [646/800], Loss: 0.0196\n",
            "Epoch [647/800], Loss: 0.0178\n",
            "Epoch [648/800], Loss: 0.0189\n",
            "Epoch [649/800], Loss: 0.0166\n",
            "Epoch [650/800], Loss: 0.0189\n",
            "Epoch [651/800], Loss: 0.0164\n",
            "Epoch [652/800], Loss: 0.0162\n",
            "Epoch [653/800], Loss: 0.0174\n",
            "Epoch [654/800], Loss: 0.0214\n",
            "Epoch [655/800], Loss: 0.0177\n",
            "Epoch [656/800], Loss: 0.0191\n",
            "Epoch [657/800], Loss: 0.0135\n",
            "Epoch [658/800], Loss: 0.0168\n",
            "Epoch [659/800], Loss: 0.0113\n",
            "Epoch [660/800], Loss: 0.0146\n",
            "Epoch [661/800], Loss: 0.0166\n",
            "Epoch [662/800], Loss: 0.0155\n",
            "Epoch [663/800], Loss: 0.0185\n",
            "Epoch [664/800], Loss: 0.0234\n",
            "Epoch [665/800], Loss: 0.0177\n",
            "Epoch [666/800], Loss: 0.0223\n",
            "Epoch [667/800], Loss: 0.0185\n",
            "Epoch [668/800], Loss: 0.0206\n",
            "Epoch [669/800], Loss: 0.0098\n",
            "Epoch [670/800], Loss: 0.0180\n",
            "Epoch [671/800], Loss: 0.0186\n",
            "Epoch [672/800], Loss: 0.0161\n",
            "Epoch [673/800], Loss: 0.0139\n",
            "Epoch [674/800], Loss: 0.0120\n",
            "Epoch [675/800], Loss: 0.0155\n",
            "Epoch [676/800], Loss: 0.0198\n",
            "Epoch [677/800], Loss: 0.0200\n",
            "Epoch [678/800], Loss: 0.0147\n",
            "Epoch [679/800], Loss: 0.0207\n",
            "Epoch [680/800], Loss: 0.0184\n",
            "Epoch [681/800], Loss: 0.0197\n",
            "Epoch [682/800], Loss: 0.0160\n",
            "Epoch [683/800], Loss: 0.0208\n",
            "Epoch [684/800], Loss: 0.0182\n",
            "Epoch [685/800], Loss: 0.0122\n",
            "Epoch [686/800], Loss: 0.0169\n",
            "Epoch [687/800], Loss: 0.0223\n",
            "Epoch [688/800], Loss: 0.0170\n",
            "Epoch [689/800], Loss: 0.0189\n",
            "Epoch [690/800], Loss: 0.0165\n",
            "Epoch [691/800], Loss: 0.0156\n",
            "Epoch [692/800], Loss: 0.0188\n",
            "Epoch [693/800], Loss: 0.0189\n",
            "Epoch [694/800], Loss: 0.0141\n",
            "Epoch [695/800], Loss: 0.0161\n",
            "Epoch [696/800], Loss: 0.0152\n",
            "Epoch [697/800], Loss: 0.0197\n",
            "Epoch [698/800], Loss: 0.0198\n",
            "Epoch [699/800], Loss: 0.0162\n",
            "Epoch [700/800], Loss: 0.0168\n",
            "Epoch [701/800], Loss: 0.0192\n",
            "Epoch [702/800], Loss: 0.0186\n",
            "Epoch [703/800], Loss: 0.0155\n",
            "Epoch [704/800], Loss: 0.0199\n",
            "Epoch [705/800], Loss: 0.0208\n",
            "Epoch [706/800], Loss: 0.0245\n",
            "Epoch [707/800], Loss: 0.0167\n",
            "Epoch [708/800], Loss: 0.0155\n",
            "Epoch [709/800], Loss: 0.0152\n",
            "Epoch [710/800], Loss: 0.0169\n",
            "Epoch [711/800], Loss: 0.0196\n",
            "Epoch [712/800], Loss: 0.0176\n",
            "Epoch [713/800], Loss: 0.0191\n",
            "Epoch [714/800], Loss: 0.0215\n",
            "Epoch [715/800], Loss: 0.0186\n",
            "Epoch [716/800], Loss: 0.0158\n",
            "Epoch [717/800], Loss: 0.0171\n",
            "Epoch [718/800], Loss: 0.0152\n",
            "Epoch [719/800], Loss: 0.0171\n",
            "Epoch [720/800], Loss: 0.0184\n",
            "Epoch [721/800], Loss: 0.0170\n",
            "Epoch [722/800], Loss: 0.0179\n",
            "Epoch [723/800], Loss: 0.0150\n",
            "Epoch [724/800], Loss: 0.0247\n",
            "Epoch [725/800], Loss: 0.0160\n",
            "Epoch [726/800], Loss: 0.0190\n",
            "Epoch [727/800], Loss: 0.0151\n",
            "Epoch [728/800], Loss: 0.0136\n",
            "Epoch [729/800], Loss: 0.0194\n",
            "Epoch [730/800], Loss: 0.0172\n",
            "Epoch [731/800], Loss: 0.0181\n",
            "Epoch [732/800], Loss: 0.0205\n",
            "Epoch [733/800], Loss: 0.0160\n",
            "Epoch [734/800], Loss: 0.0139\n",
            "Epoch [735/800], Loss: 0.0173\n",
            "Epoch [736/800], Loss: 0.0139\n",
            "Epoch [737/800], Loss: 0.0183\n",
            "Epoch [738/800], Loss: 0.0206\n",
            "Epoch [739/800], Loss: 0.0176\n",
            "Epoch [740/800], Loss: 0.0171\n",
            "Epoch [741/800], Loss: 0.0166\n",
            "Epoch [742/800], Loss: 0.0171\n",
            "Epoch [743/800], Loss: 0.0162\n",
            "Epoch [744/800], Loss: 0.0169\n",
            "Epoch [745/800], Loss: 0.0188\n",
            "Epoch [746/800], Loss: 0.0170\n",
            "Epoch [747/800], Loss: 0.0162\n",
            "Epoch [748/800], Loss: 0.0211\n",
            "Epoch [749/800], Loss: 0.0175\n",
            "Epoch [750/800], Loss: 0.0173\n",
            "Epoch [751/800], Loss: 0.0142\n",
            "Epoch [752/800], Loss: 0.0189\n",
            "Epoch [753/800], Loss: 0.0192\n",
            "Epoch [754/800], Loss: 0.0179\n",
            "Epoch [755/800], Loss: 0.0196\n",
            "Epoch [756/800], Loss: 0.0155\n",
            "Epoch [757/800], Loss: 0.0175\n",
            "Epoch [758/800], Loss: 0.0184\n",
            "Epoch [759/800], Loss: 0.0106\n",
            "Epoch [760/800], Loss: 0.0135\n",
            "Epoch [761/800], Loss: 0.0167\n",
            "Epoch [762/800], Loss: 0.0170\n",
            "Epoch [763/800], Loss: 0.0175\n",
            "Epoch [764/800], Loss: 0.0127\n",
            "Epoch [765/800], Loss: 0.0117\n",
            "Epoch [766/800], Loss: 0.0200\n",
            "Epoch [767/800], Loss: 0.0155\n",
            "Epoch [768/800], Loss: 0.0144\n",
            "Epoch [769/800], Loss: 0.0224\n",
            "Epoch [770/800], Loss: 0.0173\n",
            "Epoch [771/800], Loss: 0.0184\n",
            "Epoch [772/800], Loss: 0.0207\n",
            "Epoch [773/800], Loss: 0.0179\n",
            "Epoch [774/800], Loss: 0.0175\n",
            "Epoch [775/800], Loss: 0.0152\n",
            "Epoch [776/800], Loss: 0.0140\n",
            "Epoch [777/800], Loss: 0.0174\n",
            "Epoch [778/800], Loss: 0.0177\n",
            "Epoch [779/800], Loss: 0.0142\n",
            "Epoch [780/800], Loss: 0.0171\n",
            "Epoch [781/800], Loss: 0.0168\n",
            "Epoch [782/800], Loss: 0.0174\n",
            "Epoch [783/800], Loss: 0.0202\n",
            "Epoch [784/800], Loss: 0.0150\n",
            "Epoch [785/800], Loss: 0.0169\n",
            "Epoch [786/800], Loss: 0.0152\n",
            "Epoch [787/800], Loss: 0.0193\n",
            "Epoch [788/800], Loss: 0.0178\n",
            "Epoch [789/800], Loss: 0.0147\n",
            "Epoch [790/800], Loss: 0.0157\n",
            "Epoch [791/800], Loss: 0.0157\n",
            "Epoch [792/800], Loss: 0.0150\n",
            "Epoch [793/800], Loss: 0.0191\n",
            "Epoch [794/800], Loss: 0.0186\n",
            "Epoch [795/800], Loss: 0.0177\n",
            "Epoch [796/800], Loss: 0.0154\n",
            "Epoch [797/800], Loss: 0.0150\n",
            "Epoch [798/800], Loss: 0.0180\n",
            "Epoch [799/800], Loss: 0.0181\n",
            "Epoch [800/800], Loss: 0.0163\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate the model\n",
        "train_model(model, train_loader, criterion, optimizer, num_epochs=NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SgWLXkusGnr",
        "outputId": "43abe9e8-557f-4632-c5e5-8a03ca71c9da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión del Modelo en Data de Prueba: 58.85%\n",
            "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
            "[neptune] [info   ] Done!\n",
            "[neptune] [info   ] Waiting for the remaining 2 operations to synchronize with Neptune. Do not kill this process.\n",
            "[neptune] [info   ] All 2 operations synced, thanks for waiting!\n",
            "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/jcmachicao/continental-edusights/e/CON-69/metadata\n"
          ]
        }
      ],
      "source": [
        "results_df, accuracy = evaluate_model_v2(model, X_test_tensor, y_test_tensor)\n",
        "results_df.to_excel(data_path + f'results_{version_name}.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS2RpVxQ_sUh",
        "outputId": "0bf45c23-70aa-489a-fa76-c09ded6c879d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58.85286783042394"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmCH9mpPxWaq"
      },
      "source": [
        "https://sites.google.com/continental.edu.pe/edusights/ModVIDAD?authuser=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-wM92kc-08w"
      },
      "outputs": [],
      "source": [
        "model_name = f'edusights_20240702_model_{version_name}.pth'\n",
        "dict_name = f'edusights_20240702_state_dict_{version_name}.pth'\n",
        "torch.save(model, data_path + 'modelos/' + model_name)\n",
        "torch.save(model.state_dict(), data_path + 'modelos/' + dict_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1qjncfIPDfcQbXwql5XliZH6fztyRN39Y",
      "authorship_tag": "ABX9TyOY4cMW6Y74rXS3/KteGExi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}